<?xml version="1.0" encoding="UTF-8" standalone="yes" ?>
<!DOCTYPE bugzilla SYSTEM "https://bugzilla.redhat.com/page.cgi?id=bugzilla.dtd">

<bugzilla version="4.4.12068.1"
          urlbase="https://bugzilla.redhat.com/"
          
          maintainer="bugzilla-error-list@redhat.com"
>

    <bug>
          <bug_id>1329264</bug_id>
          
          <creation_ts>2016-04-21 09:34:00 -0400</creation_ts>
          <short_desc>lxc + lvm triggers a bug on container shutdown only with kernel 4.4.x</short_desc>
          <delta_ts>2017-04-29 07:44:02 -0400</delta_ts>
          <reporter_accessible>1</reporter_accessible>
          <cclist_accessible>1</cclist_accessible>
          <classification_id>2</classification_id>
          <classification>Fedora</classification>
          <product>Fedora</product>
          <component>kernel</component>
          <version>25</version>
          <rep_platform>Unspecified</rep_platform>
          <op_sys>Unspecified</op_sys>
          <bug_status>CLOSED</bug_status>
          <resolution>CURRENTRELEASE</resolution>
          
          
          <bug_file_loc></bug_file_loc>
          <status_whiteboard></status_whiteboard>
          <keywords></keywords>
          <priority>unspecified</priority>
          <bug_severity>unspecified</bug_severity>
          <target_milestone>---</target_milestone>
          
          
          <everconfirmed>1</everconfirmed>
          <reporter name="Davide Repetto">red</reporter>
          <assigned_to name="Kernel Maintainer List">kernel-maint</assigned_to>
          <cc>fedora-kernel-extfs</cc>
    
    
    <cc>gansalmon</cc>
    
    
    <cc>itamar</cc>
    
    
    <cc>jonathan</cc>
    
    
    <cc>kernel-maint</cc>
    
    
    <cc>madhu.chinakonda</cc>
    
    
    <cc>mchehab</cc>
    
    
    <cc>red</cc>
          <qa_contact name="Fedora Extras Quality Assurance">extras-qa</qa_contact>
          
          <cf_fixed_in></cf_fixed_in>
          <cf_doc_type>Bug Fix</cf_doc_type>
          <cf_release_notes></cf_release_notes>
          <cf_story_points>---</cf_story_points>
          
          <cf_environment></cf_environment>
          <cf_last_closed>2017-04-28 13:22:39</cf_last_closed>
          <cf_type>Bug</cf_type>
          <cf_regression_status>---</cf_regression_status>
          <cf_mount_type>---</cf_mount_type>
          <cf_documentation_action>---</cf_documentation_action>
          <cf_crm></cf_crm>
          <cf_verified_branch></cf_verified_branch>
          <cf_category>---</cf_category>
          <cf_ovirt_team>---</cf_ovirt_team>
          
          <cf_cloudforms_team>---</cf_cloudforms_team>
          
          
          
          
          <target_release>---</target_release>
          
          <votes>0</votes>

      

      

      

          <comment_sort_order>oldest_to_newest</comment_sort_order>  
          <long_desc isprivate="0" >
    <commentid>9257849</commentid>
    <comment_count>0</comment_count>
    <who name="Davide Repetto">red</who>
    <bug_when>2016-04-21 09:34:25 -0400</bug_when>
    <thetext>Description of problem:
=======================
An LXC container configured to mount LVM volumes as its root and /home will trigger some sort of bug on exit. Probably while unmounting the LVM partition something also destroys the LVM device or something of the sort.
The result after that is that the *host* machine will not even be able to reboot or complete a &quot;ps ax&quot; command and the remote login via SSH wouldn&apos;t work anymore.

This is a configuration I&apos;ve been using successfully for years (in order to support user quotas inside LXC containers) and up to kernel-4.3.5-300.fc23.x86_64 things have worked quite well indeed.

This is how filesystems are declared in the LXC&apos;s fstab of the container:
=========================================================================  
/dev/mapper/machine_root      /lxc/machine/rootfs            ext4        defaults,noatime                              1 1
/dev/mapper/machine_home      /lxc/machine/rootfs/home       ext4        defaults,usrquota,grpquota,noatime,data=writeback,commit=60,barrier=0            1 2

Version-Release number of selected component (if applicable):
=============================================================
Tasted with:
kernel.x86_64 4.4.4-301.fc23
kernel.x86_64 4.4.6-301.fc23

How reproducible:
=================
Always

Steps to Reproduce:
===================
1. lxc-stop -n container

Actual results:
===============
Something like this (in the logs) with the HOST OS left in a crippled state:
----------------------------------------------------------------------------
[mar apr 19 18:06:08 2016] VFS: Busy inodes after unmount of dm-0. Self-destruct in 5 seconds.  Have a nice day...
[mar apr 19 18:06:08 2016] BUG: unable to handle kernel NULL pointer dereference at 00000000000001f8
[mar apr 19 18:06:08 2016] IP: [&lt;ffffffff812c19bd&gt;] ext4_evict_inode+0x2d/0x4e0
[mar apr 19 18:06:08 2016] PGD 0
[mar apr 19 18:06:08 2016] Oops: 0000 [#1] SMP
[mar apr 19 18:06:08 2016] Modules linked in: rpcsec_gss_krb5 nfsv4 dns_resolver nfs fscache xt_state binfmt_misc veth bridge stp llc nf_conntrack_ftp nf_conntrack_ipv4 nf_defrag_ipv4 xt_conntrack nf_conntrack raid10 intel_rapl iosf_mbi iTCO_wdt iTCO_vendor_support x86_pkg_temp_thermal mgag200 coretemp ttm drm_kms_helper kvm_intel ipmi_devintf ppdev drm kvm joydev ipmi_ssif int3403_thermal lpc_ich irqbypass crct10dif_pclmul crc32_pclmul int3402_thermal int340x_thermal_zone video winbond_cir rc_core parport_pc shpchp parport i2c_i801 ie31200_edac ipmi_si ipmi_msghandler edac_core tpm_tis tpm int3400_thermal acpi_thermal_rel nfsd auth_rpcgss nfs_acl lockd grace sunrpc raid1 igb crc32c_intel i2c_algo_bit dca ptp pps_core fjes
[mar apr 19 18:06:08 2016] CPU: 1 PID: 1513 Comm: lxc-autostart Not tainted 4.4.6-301.fc23.x86_64 #1
[mar apr 19 18:06:08 2016] Hardware name: Intel Corporation S1200RP/S1200RP, BIOS S1200RP.86B.03.02.0003.070120151022 07/01/2015
[mar apr 19 18:06:08 2016] task: ffff88082680bc00 ti: ffff88082203c000 task.ti: ffff88082203c000
[mar apr 19 18:06:08 2016] RIP: 0010:[&lt;ffffffff812c19bd&gt;]  [&lt;ffffffff812c19bd&gt;] ext4_evict_inode+0x2d/0x4e0
[mar apr 19 18:06:08 2016] RSP: 0018:ffff88082203fd30  EFLAGS: 00010202
[mar apr 19 18:06:08 2016] RAX: ffff88081f855800 RBX: ffff8808185244c0 RCX: 0000000000000034
[mar apr 19 18:06:08 2016] RDX: 0000000000000000 RSI: 0000000000000007 RDI: ffff8808185244c0
[mar apr 19 18:06:08 2016] RBP: ffff88082203fd40 R08: ffff8808185245e0 R09: 0000000000000000
[mar apr 19 18:06:08 2016] R10: 0000000000000019 R11: 0000000000850000 R12: ffff8808185244c0
[mar apr 19 18:06:08 2016] R13: ffffffff818336e0 R14: ffff880818524548 R15: ffffffff81d3af80
[mar apr 19 18:06:08 2016] FS:  00007f42a1988840(0000) GS:ffff88082f440000(0000) knlGS:0000000000000000
[mar apr 19 18:06:08 2016] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[mar apr 19 18:06:08 2016] CR2: 00000000000001f8 CR3: 000000081f4bd000 CR4: 00000000001406e0
[mar apr 19 18:06:08 2016] Stack:
[mar apr 19 18:06:08 2016]  ffff8808185244c0 ffff8808185245e0 ffff88082203fd68 ffffffff81248a1a
[mar apr 19 18:06:08 2016]  ffff88081f855800 ffff8808185244c0 ffffffff818336e0 ffff88082203fda0
[mar apr 19 18:06:08 2016]  ffffffff81248cee ffff88082b411800 ffff880829ea9800 0000000000000000
[mar apr 19 18:06:08 2016] Call Trace:
[mar apr 19 18:06:08 2016]  [&lt;ffffffff81248a1a&gt;] evict+0xaa/0x170
[mar apr 19 18:06:08 2016]  [&lt;ffffffff81248cee&gt;] iput+0x1be/0x240
[mar apr 19 18:06:08 2016]  [&lt;ffffffff812b316d&gt;] devpts_del_ref+0x2d/0x40
[mar apr 19 18:06:08 2016]  [&lt;ffffffff8149e166&gt;] pty_unix98_shutdown+0x36/0x50
[mar apr 19 18:06:08 2016]  [&lt;ffffffff81495037&gt;] release_tty+0x37/0xf0
[mar apr 19 18:06:08 2016]  [&lt;ffffffff814954fd&gt;] tty_release+0x40d/0x560
[mar apr 19 18:06:08 2016]  [&lt;ffffffff8122fc3c&gt;] __fput+0xdc/0x1e0
[mar apr 19 18:06:08 2016]  [&lt;ffffffff8122fd7e&gt;] ____fput+0xe/0x10
[mar apr 19 18:06:08 2016]  [&lt;ffffffff810c0b13&gt;] task_work_run+0x73/0x90
[mar apr 19 18:06:08 2016]  [&lt;ffffffff81003242&gt;] exit_to_usermode_loop+0xc2/0xd0
[mar apr 19 18:06:08 2016]  [&lt;ffffffff81003d31&gt;] syscall_return_slowpath+0xa1/0xb0
[mar apr 19 18:06:08 2016]  [&lt;ffffffff817a070c&gt;] int_ret_from_sys_call+0x25/0x8f
[mar apr 19 18:06:08 2016] Code: 44 00 00 55 48 89 e5 41 54 53 49 89 fc 0f 1f 44 00 00 41 8b 44 24 48 85 c0 0f 84 b9 00 00 00 49 8b 44 24 28 48 8b 90 58 04 00 00 &lt;48&gt; 8b ba f8 01 00 00 48 85 ff 74 25 41 0f b7 04 24 89 c1 66 81
[mar apr 19 18:06:08 2016] RIP  [&lt;ffffffff812c19bd&gt;] ext4_evict_inode+0x2d/0x4e0
[mar apr 19 18:06:08 2016]  RSP &lt;ffff88082203fd30&gt;
[mar apr 19 18:06:08 2016] CR2: 00000000000001f8
[mar apr 19 18:06:08 2016] ---[ end trace 91f7697a53f02b20 ]---


Expected results:
=================
Container cleanly shutdown and the host os still in tip-top shape.

Additional info:
================
The guest OS inside the lxc machines in use with that system varies between.
CentOS release 5.11 (Final) and CentOS release 6.7 (Final)

Relevant bits of the LXC config files of one of the containers:
---------------------------------------------------------------
lxc.mount = /lxc/machine/fstab

lxc.cgroup.devices.allow = c *:* m
lxc.cgroup.devices.allow = b *:* m

lxc.cgroup.devices.allow = c 7:23 rwm
lxc.cgroup.devices.allow = c 253:1 rwm
lxc.cgroup.devices.allow = c 253:0 rwm

lxc.cgroup.devices.allow = b 7:23 rwm
lxc.cgroup.devices.allow = b 253:1 rwm
lxc.cgroup.devices.allow = b 253:0 rwm

lxc.autodev = 1
lxc.kmsg = 0
lxc.start.auto = 1
lxc.start.delay = 5
lxc.start.order = 10

lxc.cap.drop =
lxc.cap.drop = mac_admin mac_override
lxc.cap.drop = sys_module sys_nice sys_pacct
lxc.cap.drop = sys_rawio sys_time</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9365479</commentid>
    <comment_count>1</comment_count>
    <who name="Josh Boyer">jwboyer</who>
    <bug_when>2016-05-27 09:24:26 -0400</bug_when>
    <thetext>Is this still happening with a 4.5.y kernel?</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9400471</commentid>
    <comment_count>2</comment_count>
    <who name="Davide Repetto">red</who>
    <bug_when>2016-06-09 05:58:42 -0400</bug_when>
    <thetext>Yes, it still happens with kernel 4.5.6-300.

Sorry it took so long Josh, but I couldn&apos;t test on production machines so I had to take the time to rough up a test machine in my lab.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9739119</commentid>
    <comment_count>3</comment_count>
    <who name="Laura Abbott">labbott</who>
    <bug_when>2016-09-23 15:34:02 -0400</bug_when>
    <thetext>*********** MASS BUG UPDATE **************
 
We apologize for the inconvenience.  There is a large number of bugs to go through and several of them have gone stale.  Due to this, we are doing a mass bug update across all of the Fedora 23 kernel bugs.
 
Fedora 23 has now been rebased to 4.7.4-100.fc23.  Please test this kernel update (or newer) and let us know if you issue has been resolved or if it is still present with the newer kernel.
 
If you have moved on to Fedora 24 or 25, and are still experiencing this issue, please change the version to Fedora 24 or 25.
 
If you experience different issues, please open a new bug report for those.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9768874</commentid>
    <comment_count>4</comment_count>
    <who name="Davide Repetto">red</who>
    <bug_when>2016-10-05 13:46:14 -0400</bug_when>
    <thetext>In the meantime I upgraded the machine to Fedora24 and the bug is still happening; now, with kernel-4.7.5-200.fc24.x86_64.

Last working kernel is kernel-4.3.5-300.fc23.x86_64 (which I&apos;m still using)</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10324922</commentid>
    <comment_count>5</comment_count>
    <who name="Justin M. Forbes">jforbes</who>
    <bug_when>2017-04-11 10:47:33 -0400</bug_when>
    <thetext>*********** MASS BUG UPDATE **************

We apologize for the inconvenience.  There are a large number of bugs to go through and several of them have gone stale.  Due to this, we are doing a mass bug update across all of the Fedora 24 kernel bugs.

Fedora 25 has now been rebased to 4.10.9-100.fc24.  Please test this kernel update (or newer) and let us know if you issue has been resolved or if it is still present with the newer kernel.

If you have moved on to Fedora 26, and are still experiencing this issue, please change the version to Fedora 26.

If you experience different issues, please open a new bug report for those.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10373347</commentid>
    <comment_count>6</comment_count>
    <who name="Justin M. Forbes">jforbes</who>
    <bug_when>2017-04-28 13:22:39 -0400</bug_when>
    <thetext>*********** MASS BUG UPDATE **************
This bug is being closed with INSUFFICIENT_DATA as there has not been a response in 2 weeks. If you are still experiencing this issue, please reopen and attach the 
relevant data from the latest kernel you are running and any data that might have been requested previously.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10374244</commentid>
    <comment_count>7</comment_count>
    <who name="Davide Repetto">red</who>
    <bug_when>2017-04-29 07:43:13 -0400</bug_when>
    <thetext>Bug Currently fixed with kernel 4.10.12-200.fc25.</thetext>
  </long_desc>
      
      

    </bug>

</bugzilla>