<?xml version="1.0" encoding="UTF-8" standalone="yes" ?>
<!DOCTYPE bugzilla SYSTEM "https://bugzilla.redhat.com/page.cgi?id=bugzilla.dtd">

<bugzilla version="4.4.12068.1"
          urlbase="https://bugzilla.redhat.com/"
          
          maintainer="bugzilla-error-list@redhat.com"
>

    <bug>
          <bug_id>1359975</bug_id>
          
          <creation_ts>2016-07-26 00:00:00 -0400</creation_ts>
          <short_desc>pmchart run-away mem leak replaying multi-archive when rewinding</short_desc>
          <delta_ts>2017-01-06 18:48:39 -0500</delta_ts>
          <reporter_accessible>1</reporter_accessible>
          <cclist_accessible>1</cclist_accessible>
          <classification_id>2</classification_id>
          <classification>Fedora</classification>
          <product>Fedora</product>
          <component>pcp</component>
          <version>24</version>
          <rep_platform>Unspecified</rep_platform>
          <op_sys>Unspecified</op_sys>
          <bug_status>CLOSED</bug_status>
          <resolution>ERRATA</resolution>
          
          
          <bug_file_loc></bug_file_loc>
          <status_whiteboard></status_whiteboard>
          <keywords></keywords>
          <priority>unspecified</priority>
          <bug_severity>urgent</bug_severity>
          <target_milestone>---</target_milestone>
          
          
          <everconfirmed>1</everconfirmed>
          <reporter name="Mark Goodwin">mgoodwin</reporter>
          <assigned_to name="Dave Brolley">brolley</assigned_to>
          <cc>brolley</cc>
    
    
    <cc>fche</cc>
    
    
    <cc>lberk</cc>
    
    
    <cc>mbenitez</cc>
    
    
    <cc>mgoodwin</cc>
    
    
    <cc>nathans</cc>
    
    
    <cc>pcp</cc>
    
    
    <cc>scox</cc>
          <qa_contact name="Fedora Extras Quality Assurance">extras-qa</qa_contact>
          
          <cf_fixed_in>3.11.7-1 pcp-3.11.7-1.fc25 pcp-3.11.7-1.fc24 pcp-3.11.7-1.el5</cf_fixed_in>
          <cf_doc_type>If docs needed, set a value</cf_doc_type>
          <cf_release_notes></cf_release_notes>
          <cf_story_points>---</cf_story_points>
          
          <cf_environment></cf_environment>
          <cf_last_closed>2017-01-02 14:50:59</cf_last_closed>
          <cf_type>Bug</cf_type>
          <cf_regression_status>---</cf_regression_status>
          <cf_mount_type>---</cf_mount_type>
          <cf_documentation_action>---</cf_documentation_action>
          <cf_crm></cf_crm>
          <cf_verified_branch></cf_verified_branch>
          <cf_category>---</cf_category>
          <cf_ovirt_team>---</cf_ovirt_team>
          
          <cf_cloudforms_team>---</cf_cloudforms_team>
          
          
          
          
          <target_release>---</target_release>
          
          <votes>0</votes>

      

      

      

          <comment_sort_order>oldest_to_newest</comment_sort_order>  
          <long_desc isprivate="0" >
    <commentid>9548645</commentid>
    <comment_count>0</comment_count>
    <who name="Mark Goodwin">mgoodwin</who>
    <bug_when>2016-07-26 00:00:50 -0400</bug_when>
    <thetext>Description of problem: pmchart goes into a run-away memory leak and eventually gets killed by oom killer

Version-Release number of selected component (if applicable): pcp-3.11.4-1 / F24

How reproducible: easily

Steps to Reproduce:
1. wget http://people.redhat.com/~mgoodwin/pcp/archives/goody.tgz; tar xzf goody.tgz
2. pmchart -z -a goody -t 10m -O-0 -s 400 -v 400 -geometry 800x450 -c CPU
3. expose time controls, select FAST, press REWIND then press STOP

Actual results: pmtime dialog stops, but pmchart does not scroll and spins out of control and is eventually killed by oom.

Expected results: pmchart should scroll and stop as per pmtime

Additional info: this only occurs when replaying multiple archives (in the directory &apos;goody&apos;). It is not reproducible if those archives are merged with pmlogextract and we only open the one merged archive.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9930810</commentid>
    <comment_count>1</comment_count>
    <who name="Mark Goodwin">mgoodwin</who>
    <bug_when>2016-11-25 17:58:49 -0500</bug_when>
    <thetext>This is still easily reproducible with pmchart from pcp-3.11.6 and latest from the current 3.11.7, as per the Steps to Reproduce in Comment #1

I re-tested since pmchart has had some changes recently. I guess maybe the priority should be bumped up a bit since it basically kills your desktop if you hit it (until the oom killer steps in to stop it).</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9930833</commentid>
    <comment_count>2</comment_count>
    <who name="Mark Goodwin">mgoodwin</who>
    <bug_when>2016-11-25 18:21:43 -0500</bug_when>
    <thetext>Here&apos;s an ltrace of the repeating library call sequence - this repeats until the system eventually runs out of memory and the pmchart process is oom/killed

10440 pmFetch(7, 0x11e8866b0, 0x7ffc4a5b02c0, 7) = 0
10440 QVectorData::free(QVectorData*, int)(0x11e8866a0, 8, 0x149c400, 0) = 0
10440 __pmtimevalSub(0x3616fa0, 0x3616fb0, 0x62b2c, 0) = 0
10440 pmFreeResult(0x11e8866e0, 0x7ffc4a5b01c0, 7, 0) = 0
10440 pmUseContext(0, 1, 0x3616fd0, 0)           = 0
10440 pmCtime(0x7ffc4a5b0358, 0x7677a0, 0x7f89f42141e0, 0) = 0x7677a0
10440 strlen(&quot;Mon Jul 18 13:53:12 2016\n&quot;)       = 25
10440 qQNaN()(1, 0xffffcfc1, 1, 0x3685400)       = 0x3689d10
10440 memmove(0x42c1560, &quot;DDDDD\304$@\0\0\0\0\0\0\370\377\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x42c1560
10440 memmove(0x42c9570, &quot;\256&amp;\026;\264\242\320?\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x42c9570
10440 qQNaN()(1, 0xffffcfc1, 1, 0x3685400)       = 0x36aa820
10440 memmove(0x42d1580, &quot;DDDDDD\020@\0\0\0\0\0\0\370\377\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x42d1580
10440 memmove(0x370b360, &quot;\342\306o\264\243&amp;\327?\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x370b360
10440 qQNaN()(1, 0xffffcfc1, 1, 0x3685400)       = 0x36a7340
10440 memmove(0x42d9590, &quot;\0\0\0\0\0\0\0\0\0\0\0\0\0\0\370\377\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x42d9590
10440 memmove(0x36f0bc0, &quot;\342\306o\264\243&amp;\327?\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x36f0bc0
10440 qQNaN()(1, 0xffffcfc1, 1, 0x3685400)       = 0x36ba4d0
10440 memmove(0x3713380, &quot;\021\021\021\021\021\021\221?\0\0\0\0\0\0\370\377\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x3713380
10440 memmove(0x42e15a0, &quot;\227\310}\253y-\327?\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x42e15a0
10440 qQNaN()(1, 0xffffcfc1, 1, 0x3685400)       = 0x36bfe20
10440 memmove(0x42e95b0, &quot;UUUUUU&quot;@\0\0\0\0\0\0\370\377\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x42e95b0
10440 memmove(0x371b3a0, &quot;`\271\3129\225\356\342?\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x371b3a0
10440 qQNaN()(1, 0xffffcfc1, 1, 0x3685400)       = 0x36caeb0
10440 memmove(0x42f15c0, &quot;\0\0\0\0\0\0\0\0\0\0\0\0\0\0\370\377\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x42f15c0
10440 memmove(0x36f8be0, &quot;`\271\3129\225\356\342?\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x36f8be0
10440 qQNaN()(1, 0xffffcfc1, 1, 0x3685400)       = 0x36c7730
10440 memmove(0x42f95d0, &quot;~\261\344\027\v\006\257@\0\0\0\0\0\0\370\377\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x42f95d0
10440 memmove(0x37233c0, &quot;\0\0\0\0\0\0Y@\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x37233c0
10440 qIsNaN(double)(0x366f350, 0xffffffff, 1, 5) = 0
10440 qIsNaN(double)(0x36a8610, 0xffffffff, 1, 0) = 0
10440 qIsNaN(double)(0x36b3b70, 0xffffffff, 1, 0) = 0
10440 qIsNaN(double)(0x36bb680, 0xffffffff, 1, 0) = 0
10440 qIsNaN(double)(0x36c1f70, 0xffffffff, 1, 0) = 0
10440 qIsNaN(double)(0x36ca620, 0xffffffff, 1, 0) = 0
10440 qIsNaN(double)(0x36d1a70, 0xffffffff, 1, 0) = 0
10440 qIsNaN(double)(0x366f350, 0xffffffff, 1, 0) = 0
10440 qIsNaN(double)(0x36a8610, 0xffffffff, 1, 1) = 0
10440 qIsNaN(double)(0x36b3b70, 0xffffffff, 1, 1) = 0
10440 qIsNaN(double)(0x36bb680, 0xffffffff, 1, 1) = 0
10440 qIsNaN(double)(0x36c1f70, 0xffffffff, 1, 1) = 0
10440 qIsNaN(double)(0x36ca620, 0xffffffff, 1, 1) = 0
10440 qIsNaN(double)(0x36d1a70, 0xffffffff, 1, 1) = 0
10440 __pmtimevalFromReal(0x7ffc4a5b03d0, 1, 1, 1) = 0
10440 pmUseContext(0, 1, 0x3616f60, 0x7ffc4a5b0320) = 0
10440 pmSetMode(0x1030001, 0x7ffc4a5b03d0, 0xfffffda8, 0) = 0
10440 pmUseContext(0, 0, 0x3616fd0, 0)           = 0
10440 pmCtime(0x7ffc4a5b0358, 0x7677a0, 0x7f89f42141e0, 0) = 0x7677a0
10440 strlen(&quot;Mon Jul 18 13:43:12 2016\n&quot;)       = 25
10440 pmUseContext(0, 0xffffcfc1, 0, 0)          = 0
10440 QVectorData::allocate(int, int)(44, 8, 0, 0) = 0x11f80d620
10440 qMemSet(void*, int, unsigned long)(0x11f80d630, 0, 28, 0) = 0x11f80d630

-- pause, then the cycle repeats


10440 pmFetch(7, 0x11f80d630, 0x7ffc4a5b02c0, 7) = 0
10440 QVectorData::free(QVectorData*, int)(0x11f80d620, 8, 0x149c400, 0) = 0
10440 __pmtimevalSub(0x3616fa0, 0x3616fb0, 0x62b2c, 0) = 0
10440 pmExtractValue(1, 0x11ec7bf30, 3, 0x7ffc4a5b01d0) = 0
10440 pmExtractValue(1, 0x11ec7c1b0, 3, 0x7ffc4a5b01d0) = 0
10440 pmExtractValue(1, 0x11ec7c200, 3, 0x7ffc4a5b01d0) = 0
10440 pmExtractValue(1, 0x11ec7c250, 3, 0x7ffc4a5b01d0) = 0
10440 pmExtractValue(1, 0x11ec7c2a0, 3, 0x7ffc4a5b01d0) = 0
10440 pmExtractValue(1, 0x11ec7c2f0, 3, 0x7ffc4a5b01d0) = 0
10440 pmExtractValue(1, 0x11ec7c340, 3, 0x7ffc4a5b01d0) = 0
10440 pmFreeResult(0x11f80d660, 0x7f89f3ffa6d8, 7, 0) = 0
10440 pmUseContext(0, 1, 0x3616fd0, 0)           = 0
10440 pmCtime(0x7ffc4a5b0358, 0x7677a0, 0x7f89f42141e0, 0) = 0x7677a0
10440 strlen(&quot;Mon Jul 18 13:43:12 2016\n&quot;)       = 25
10440 qQNaN()(1, 0xffffcfc1, 1, 0x3685400)       = 0x3689d10
10440 memmove(0x42c1560, &quot;\0\0\0\0\0\0\370\377\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x42c1560
10440 memmove(0x42c9570, &quot;\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x42c9570
10440 qQNaN()(1, 0xffffcfc1, 1, 0x3685400)       = 0x36aa820
10440 memmove(0x42d1580, &quot;\0\0\0\0\0\0\370\377\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x42d1580
10440 memmove(0x370b360, &quot;\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x370b360
10440 qQNaN()(1, 0xffffcfc1, 1, 0x3685400)       = 0x36a7340
10440 memmove(0x42d9590, &quot;\0\0\0\0\0\0\370\377\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x42d9590
10440 memmove(0x36f0bc0, &quot;\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x36f0bc0
10440 qQNaN()(1, 0xffffcfc1, 1, 0x3685400)       = 0x36ba4d0
10440 memmove(0x3713380, &quot;\0\0\0\0\0\0\370\377\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x3713380
10440 memmove(0x42e15a0, &quot;\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x42e15a0
10440 qQNaN()(1, 0xffffcfc1, 1, 0x3685400)       = 0x36bfe20
10440 memmove(0x42e95b0, &quot;\0\0\0\0\0\0\370\377\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x42e95b0
10440 memmove(0x371b3a0, &quot;\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x371b3a0
10440 qQNaN()(1, 0xffffcfc1, 1, 0x3685400)       = 0x36caeb0
10440 memmove(0x42f15c0, &quot;\0\0\0\0\0\0\370\377\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x42f15c0
10440 memmove(0x36f8be0, &quot;\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x36f8be0
10440 qQNaN()(1, 0xffffcfc1, 1, 0x3685400)       = 0x36c7730
10440 memmove(0x42f95d0, &quot;\0\0\0\0\0\0\370\377\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x42f95d0
10440 memmove(0x37233c0, &quot;\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;..., 25536) = 0x37233c0
10440 qIsNaN(double)(0x366f350, 0xffffffff, 1, 5) = 0
10440 qIsNaN(double)(0x36a8610, 0xffffffff, 1, 0) = 0
10440 qIsNaN(double)(0x36b3b70, 0xffffffff, 1, 0) = 0
10440 qIsNaN(double)(0x36bb680, 0xffffffff, 1, 0) = 0
10440 qIsNaN(double)(0x36c1f70, 0xffffffff, 1, 0) = 0
10440 qIsNaN(double)(0x36ca620, 0xffffffff, 1, 0) = 0
10440 qIsNaN(double)(0x36d1a70, 0xffffffff, 1, 0) = 0
10440 qIsNaN(double)(0x366f350, 0xffffffff, 1, 0) = 0
10440 qIsNaN(double)(0x36a8610, 0xffffffff, 1, 1) = 0
10440 qIsNaN(double)(0x36b3b70, 0xffffffff, 1, 1) = 0
10440 qIsNaN(double)(0x36bb680, 0xffffffff, 1, 1) = 0
10440 qIsNaN(double)(0x36c1f70, 0xffffffff, 1, 1) = 0
10440 qIsNaN(double)(0x36ca620, 0xffffffff, 1, 1) = 0
10440 qIsNaN(double)(0x36d1a70, 0xffffffff, 1, 1) = 0
10440 __pmtimevalFromReal(0x7ffc4a5b03d0, 1, 1, 1) = 0
10440 pmUseContext(0, 1, 0x3616f60, 0x7ffc4a5b0320) = 0
10440 pmSetMode(0x1030001, 0x7ffc4a5b03d0, 0xfffffda8, 0) = 0
10440 pmUseContext(0, 0, 0x3616fd0, 0)           = 0
10440 pmCtime(0x7ffc4a5b0358, 0x7677a0, 0x7f89f42141e0, 0) = 0x7677a0
10440 strlen(&quot;Mon Jul 18 13:33:12 2016\n&quot;)       = 25
10440 pmUseContext(0, 0, 0, 0)                   = 0
10440 QVectorData::allocate(int, int)(44, 8, 0, 0) = 0x120794690
10440 qMemSet(void*, int, unsigned long)(0x1207946a0, 0, 28, 0) = 0x1207946a0

.. continues to repeat, as above</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9930931</commentid>
    <comment_count>3</comment_count>
    <who name="Frank Ch. Eigler">fche</who>
    <bug_when>2016-11-25 23:06:11 -0500</bug_when>
    <thetext>env PCP_DEDBUG=14336  pmchart ...  

indicates that once you hit the backwards button, the app disregards stop, and keeps on chugging.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9930938</commentid>
    <comment_count>4</comment_count>
    <who name="Frank Ch. Eigler">fche</who>
    <bug_when>2016-11-25 23:32:51 -0500</bug_when>
    <thetext>One more bit of data:

% build a copy of pmchart with debuginfo in it, if able (pcp configure --with-qt=debug one way)
% gdb -args pmchart ....
(gdb) run
... until popping up the time selector
^C
(gdb) break mmap
(gdb) cont

#0  0x00002aaaad2ed3e0 in __mmap (addr=addr@entry=0x0, len=len@entry=1048576, prot=prot@entry=3, flags=flags@entry=34, fd=fd@entry=-1, offset=offset@entry=0) at ../sysdeps/unix/sysv/linux/wordsize-64/mmap.c:33
#1  0x00002aaaad26f070 in sysmalloc (nb=nb@entry=2032, av=0x2aaaad5acb00 &lt;main_arena&gt;) at malloc.c:2515
#2  0x00002aaaad26fa44 in _int_malloc (av=av@entry=0x2aaaad5acb00 &lt;main_arena&gt;, bytes=bytes@entry=2024) at malloc.c:3825
#3  0x00002aaaad2710b0 in __GI___libc_malloc (bytes=2024) at malloc.c:2911
#4  0x00002aaaaacfed0a in __pmLogLoadMeta (lcp=lcp@entry=0xb43790) at logmeta.c:345
#5  0x00002aaaaad03320 in __pmLogOpen (name=name@entry=0xaa76c0 &quot;goody/20160719.12.55&quot;, ctxp=ctxp@entry=0xa08070)
    at logutil.c:1086
#6  0x00002aaaaace4349 in __pmFindOrOpenArchive (ctxp=ctxp@entry=0xa08070, name=0xaa76c0 &quot;goody/20160719.12.55&quot;, multi_arch=multi_arch@entry=1) at context.c:555
#7  0x00002aaaaad03779 in __pmLogChangeArchive (ctxp=ctxp@entry=0xa08070, arch=arch@entry=1) at logutil.c:2829
#8  0x00002aaaaad053c6 in __pmLogSetTime (ctxp=ctxp@entry=0xa08070) at logutil.c:2292
#9  0x00002aaaaace78b8 in pmSetMode (mode=16973825, when=0x7fffffffcf90, delta=-600) at fetch.c:264
#10 0x00000000004a38a3 in QmcGroup::setArchiveMode(int, timeval const*, int) ()
#11 0x000000000048729e in GroupControl::adjustArchiveWorldViewBackward(QmcTime::Packet*, bool) (this=0xa07e70, packet=0xaad9c0, setup=false) at groupcontrol.cpp:478

... and try a (gdb)cont a few times; many such requests come in this way.

This seems to implicate libpcp multi-archive mode time scanning.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9930942</commentid>
    <comment_count>5</comment_count>
    <who name="Mark Goodwin">mgoodwin</who>
    <bug_when>2016-11-25 23:40:02 -0500</bug_when>
    <thetext>(In reply to Frank Ch. Eigler from comment #4)
&gt;
&gt; This seems to implicate libpcp multi-archive mode time scanning.

yes, and this more or less agrees with the repeating library call sequence in Comment #2, which is leaking memory on every iteration. It only occurs in multi-archive mode.

Thanks
-- Mark</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9932758</commentid>
    <comment_count>6</comment_count>
      <attachid>1225074</attachid>
    <who name="Frank Ch. Eigler">fche</who>
    <bug_when>2016-11-27 20:47:55 -0500</bug_when>
    <thetext>Created attachment 1225074
possible reproducer

this little script seems to access the archive in a similar idiosyncratic way as pmchart does (pmSetMode calls between pmFetch).  Needs the sample multi-archive directory under /tmp/goody.

% valgrind --leak-check=full FOO.py

After a few seconds&apos; run, notice how it&apos;s getting slower and slower and slower.  Then ^C, valgrind reports:

[...]

==24643== 206,400 bytes in 10,320 blocks are definitely lost in loss record 2,497 of 2,497
==24643==    at 0x4C2BBAD: malloc (vg_replace_malloc.c:299)
==24643==    by 0xDCCFA72: __pmLogLoadMeta (logmeta.c:143)
==24643==    by 0xDCD431F: __pmLogOpen (logutil.c:1086)
==24643==    by 0xDCB5348: __pmFindOrOpenArchive (context.c:555)
==24643==    by 0xDCD4778: __pmLogChangeArchive (logutil.c:2829)
==24643==    by 0xDCD63C5: __pmLogSetTime (logutil.c:2292)
==24643==    by 0xDCB88B7: pmSetMode (fetch.c:264)
==24643==    by 0x11A20C57: ffi_call_unix64 (unix64.S:76)
==24643==    by 0x11A206B9: ffi_call (ffi64.c:525)
==24643==    by 0x1180D67E: _call_function_pointer (callproc.c:837)
==24643==    by 0x1180D67E: _ctypes_callproc (callproc.c:1180)
==24643==    by 0x11807371: PyCFuncPtr_call (_ctypes.c:3954)
==24643==    by 0x4E81FD2: PyObject_Call (abstract.c:2546)

% python FOO.py
[... runs ... and bloats memory more and more and more ...]</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9932954</commentid>
    <comment_count>7</comment_count>
      <attachid>1225100</attachid>
    <who name="Frank Ch. Eigler">fche</who>
    <bug_when>2016-11-27 22:57:59 -0500</bug_when>
    <thetext>Created attachment 1225100
fix for one small leak

One leak down with this patch.

One monster leak found with valgrind on the python reproducer, with a SIGUSR1 interrupt (to preclude libpcp cleanup on context creation):

==26053== 38,135,288 bytes in 5,072 blocks are still reachable in loss record 3,616 of 3,616
==26053==    at 0x4C2BBAD: malloc (vg_replace_malloc.c:299)
==26053==    by 0xDCD0FD7: __pmLogLoadMeta (logmeta.c:313)
==26053==    by 0xDCD567F: __pmLogOpen (logutil.c:1086)
==26053==    by 0xDCB66B8: __pmFindOrOpenArchive (context.c:555)
==26053==    by 0xDCD5AD8: __pmLogChangeArchive (logutil.c:2829)
==26053==    by 0xDCD5EC9: __pmLogChangeToPreviousArchive (logutil.c:3010)
==26053==    by 0xDCD645C: __pmLogRead (logutil.c:1617)
==26053==    by 0xDCD9DE2: cache_read (interp.c:230)
==26053==    by 0xDCDC2A7: __pmLogFetchInterp (interp.c:1034)
==26053==    by 0xDCD7609: __pmLogFetch (logutil.c:1944)
==26053==    by 0xDCB9664: pmFetch (fetch.c:154)
==26053==    by 0xDCBBA61: pmFetchGroup (fetchgroup.c:1559)

[etc]

What&apos;s happening is that as interp.c and pals scan back and forth in time across the archives, a new copies of the entire instance domains are constantly allocated and saved via addindom().  addindom() does not perform any duplicate elimination over e.g. idp-&gt;stamp, or the contents of the indom, so these copies pile up in memory at an impressive pace.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9932968</commentid>
    <comment_count>8</comment_count>
    <who name="Mark Goodwin">mgoodwin</who>
    <bug_when>2016-11-27 23:02:10 -0500</bug_when>
    <thetext>Heh, I just arrived at exactly the same patch :)  It fixes the original pmchart issue but your FOO.py repro still seems to leak (another python gc issue maybe??)</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9932971</commentid>
    <comment_count>9</comment_count>
    <who name="Frank Ch. Eigler">fche</who>
    <bug_when>2016-11-27 23:06:09 -0500</bug_when>
    <thetext>(In reply to Mark Goodwin from comment #8)
&gt; [...] (another python gc issue maybe??)

Nope, see above re. monster leak: a design problem in (multi-?) archive work.  indoms are being repeatedly stored in memory.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9932976</commentid>
    <comment_count>10</comment_count>
    <who name="Mark Goodwin">mgoodwin</who>
    <bug_when>2016-11-27 23:14:42 -0500</bug_when>
    <thetext>yes you are correct - it doesn&apos;t fix the original pmchart issue (but it does fix a leak nevertheless, so at least some progress has been made).

# pmchart -z -a goody -t 10m -O-0 -s 400 -v 400 -geometry 800x450 -c CPU
(where &apos;goody&apos; is a directory containing multiple archives)

The &apos;monster&apos; indom leak is going to need some careful thought ...

Cheers</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9951709</commentid>
    <comment_count>12</comment_count>
      <attachid>1227374</attachid>
    <who name="Dave Brolley">brolley</who>
    <bug_when>2016-12-02 10:46:21 -0500</bug_when>
    <thetext>Created attachment 1227374
Proposed patch for indom duplication

Here is a proposed patch for filtering out the duplicate indoms. It stops the runaway memory growth, but pm chart still ends up in a loop reading from the multi-archive and remains unresposive.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9958514</commentid>
    <comment_count>13</comment_count>
    <who name="Mark Goodwin">mgoodwin</who>
    <bug_when>2016-12-05 23:09:33 -0500</bug_when>
    <thetext>Hi Dave, I applied your patch from Comment #12 but it didn&apos;t go too well.
Looks like an invalid free at __pmLogLoadMeta (logmeta.c:434), which is :

    430                 /*
    431                  * This pmid is already known, and matches.  We can free the newly
    432                  * read copy and use the one in the hash table.
    433                  */
    434                 free(dp);
    435                 dp = olddp;

i.e. your new patch seems to be incompatible with the following commit which is in my tree but maybe not in yours :

commit 6ed4b2452ff24356f90d37ac658cdbae4053e8c0
Author: Frank Ch. Eigler &lt;fche@redhat.com&gt;
Date:   Thu Dec 1 15:03:01 2016 +1100

    libpcp: plug a small mem leak in __pmLogLoadMeta()
    
    Plug a leaking pmDesc in __pmLogLoadMeta() for the case where the pmid
    is already in the hash table. Partial fix for RHBZ#1359975. There are
    still other leaks involving multi-archive replay remaining for this bug.

Should we just back-out that change instead since the code has now been re-worked?


$ valgrind pmchart -a goody -O-0 -c CPU
==25136== Memcheck, a memory error detector
==25136== Copyright (C) 2002-2015, and GNU GPL&apos;d, by Julian Seward et al.
==25136== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info
==25136== Command: pmchart -a goody -O-0 -c CPU
==25136== 
==25136== Conditional jump or move depends on uninitialised value(s)
==25136==    at 0x4C2CD0D: free (vg_replace_malloc.c:530)
==25136==    by 0x4E65E7A: __pmLogLoadMeta (logmeta.c:434)
==25136==    by 0x4E6A43F: __pmLogOpen (logutil.c:1086)
==25136==    by 0x4E4B348: __pmFindOrOpenArchive (context.c:555)
==25136==    by 0x4E4CAF1: initarchive (context.c:759)
==25136==    by 0x4E4CAF1: pmNewContext (context.c:1126)
==25136==    by 0x4A603B: QmcSource::retryConnect(int, QString&amp;) (in /usr/bin/pmchart)
==25136==    by 0x4A6F16: QmcSource::QmcSource(int, QString&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x4A71C6: QmcSource::getSource(int, QString&amp;, int, bool) (in /usr/bin/pmchart)
==25136==    by 0x49C1B1: QmcGroup::use(int, QString const&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x424DB0: main (in /usr/bin/pmchart)
==25136== 
==25136== Invalid free() / delete / delete[] / realloc()
==25136==    at 0x4C2CD5A: free (vg_replace_malloc.c:530)
==25136==    by 0x4E65E7A: __pmLogLoadMeta (logmeta.c:434)
==25136==    by 0x4E6A43F: __pmLogOpen (logutil.c:1086)
==25136==    by 0x4E4B348: __pmFindOrOpenArchive (context.c:555)
==25136==    by 0x4E4CAF1: initarchive (context.c:759)
==25136==    by 0x4E4CAF1: pmNewContext (context.c:1126)
==25136==    by 0x4A603B: QmcSource::retryConnect(int, QString&amp;) (in /usr/bin/pmchart)
==25136==    by 0x4A6F16: QmcSource::QmcSource(int, QString&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x4A71C6: QmcSource::getSource(int, QString&amp;, int, bool) (in /usr/bin/pmchart)
==25136==    by 0x49C1B1: QmcGroup::use(int, QString const&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x424DB0: main (in /usr/bin/pmchart)
==25136==  Address 0x4e68af9 is in the Text segment of /usr/lib64/libpcp.so.3
==25136==    at 0x4E68AF9: __pmLogChkLabel (logutil.c:233)
==25136== 
==25136== Invalid read of size 4
==25136==    at 0x4E65C01: __pmLogLoadMeta (logmeta.c:243)
==25136==    by 0x4E6A43F: __pmLogOpen (logutil.c:1086)
==25136==    by 0x4E4B348: __pmFindOrOpenArchive (context.c:555)
==25136==    by 0x4E4CAF1: initarchive (context.c:759)
==25136==    by 0x4E4CAF1: pmNewContext (context.c:1126)
==25136==    by 0x4A603B: QmcSource::retryConnect(int, QString&amp;) (in /usr/bin/pmchart)
==25136==    by 0x4A6F16: QmcSource::QmcSource(int, QString&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x4A71C6: QmcSource::getSource(int, QString&amp;, int, bool) (in /usr/bin/pmchart)
==25136==    by 0x49C1B1: QmcGroup::use(int, QString const&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x424DB0: main (in /usr/bin/pmchart)
==25136==  Address 0x1f35e374 is 4 bytes inside an unallocated block of size 112 in arena &quot;client&quot;
==25136== 
==25136== Invalid read of size 4
==25136==    at 0x4E65C0E: __pmLogLoadMeta (logmeta.c:248)
==25136==    by 0x4E6A43F: __pmLogOpen (logutil.c:1086)
==25136==    by 0x4E4B348: __pmFindOrOpenArchive (context.c:555)
==25136==    by 0x4E4CAF1: initarchive (context.c:759)
==25136==    by 0x4E4CAF1: pmNewContext (context.c:1126)
==25136==    by 0x4A603B: QmcSource::retryConnect(int, QString&amp;) (in /usr/bin/pmchart)
==25136==    by 0x4A6F16: QmcSource::QmcSource(int, QString&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x4A71C6: QmcSource::getSource(int, QString&amp;, int, bool) (in /usr/bin/pmchart)
==25136==    by 0x49C1B1: QmcGroup::use(int, QString const&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x424DB0: main (in /usr/bin/pmchart)
==25136==  Address 0x1f35e37c is 12 bytes inside an unallocated block of size 112 in arena &quot;client&quot;
==25136== 
==25136== Invalid read of size 4
==25136==    at 0x4E65C1B: __pmLogLoadMeta (logmeta.c:253)
==25136==    by 0x4E6A43F: __pmLogOpen (logutil.c:1086)
==25136==    by 0x4E4B348: __pmFindOrOpenArchive (context.c:555)
==25136==    by 0x4E4CAF1: initarchive (context.c:759)
==25136==    by 0x4E4CAF1: pmNewContext (context.c:1126)
==25136==    by 0x4A603B: QmcSource::retryConnect(int, QString&amp;) (in /usr/bin/pmchart)
==25136==    by 0x4A6F16: QmcSource::QmcSource(int, QString&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x4A71C6: QmcSource::getSource(int, QString&amp;, int, bool) (in /usr/bin/pmchart)
==25136==    by 0x49C1B1: QmcGroup::use(int, QString const&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x424DB0: main (in /usr/bin/pmchart)
==25136==  Address 0x1f35e378 is 8 bytes inside an unallocated block of size 112 in arena &quot;client&quot;
==25136== 
==25136== Invalid read of size 1
==25136==    at 0x4E65C28: __pmLogLoadMeta (logmeta.c:258)
==25136==    by 0x4E6A43F: __pmLogOpen (logutil.c:1086)
==25136==    by 0x4E4B348: __pmFindOrOpenArchive (context.c:555)
==25136==    by 0x4E4CAF1: initarchive (context.c:759)
==25136==    by 0x4E4CAF1: pmNewContext (context.c:1126)
==25136==    by 0x4A603B: QmcSource::retryConnect(int, QString&amp;) (in /usr/bin/pmchart)
==25136==    by 0x4A6F16: QmcSource::QmcSource(int, QString&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x4A71C6: QmcSource::getSource(int, QString&amp;, int, bool) (in /usr/bin/pmchart)
==25136==    by 0x49C1B1: QmcGroup::use(int, QString const&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x424DB0: main (in /usr/bin/pmchart)
==25136==  Address 0x1f35e383 is 19 bytes inside an unallocated block of size 112 in arena &quot;client&quot;
==25136== 
==25136== Invalid read of size 1
==25136==    at 0x4E65C36: __pmLogLoadMeta (logmeta.c:260)
==25136==    by 0x4E6A43F: __pmLogOpen (logutil.c:1086)
==25136==    by 0x4E4B348: __pmFindOrOpenArchive (context.c:555)
==25136==    by 0x4E4CAF1: initarchive (context.c:759)
==25136==    by 0x4E4CAF1: pmNewContext (context.c:1126)
==25136==    by 0x4A603B: QmcSource::retryConnect(int, QString&amp;) (in /usr/bin/pmchart)
==25136==    by 0x4A6F16: QmcSource::QmcSource(int, QString&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x4A71C6: QmcSource::getSource(int, QString&amp;, int, bool) (in /usr/bin/pmchart)
==25136==    by 0x49C1B1: QmcGroup::use(int, QString const&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x424DB0: main (in /usr/bin/pmchart)
==25136==  Address 0x1f35e382 is 18 bytes inside an unallocated block of size 112 in arena &quot;client&quot;
==25136== 
==25136== Invalid read of size 1
==25136==    at 0x4E65C44: __pmLogLoadMeta (logmeta.c:262)
==25136==    by 0x4E6A43F: __pmLogOpen (logutil.c:1086)
==25136==    by 0x4E4B348: __pmFindOrOpenArchive (context.c:555)
==25136==    by 0x4E4CAF1: initarchive (context.c:759)
==25136==    by 0x4E4CAF1: pmNewContext (context.c:1126)
==25136==    by 0x4A603B: QmcSource::retryConnect(int, QString&amp;) (in /usr/bin/pmchart)
==25136==    by 0x4A6F16: QmcSource::QmcSource(int, QString&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x4A71C6: QmcSource::getSource(int, QString&amp;, int, bool) (in /usr/bin/pmchart)
==25136==    by 0x49C1B1: QmcGroup::use(int, QString const&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x424DB0: main (in /usr/bin/pmchart)
==25136==  Address 0x1f35e381 is 17 bytes inside an unallocated block of size 112 in arena &quot;client&quot;
==25136== 
==25136== Invalid read of size 4
==25136==    at 0x4E65CA7: __pmLogLoadMeta (logmeta.c:349)
==25136==    by 0x4E6A43F: __pmLogOpen (logutil.c:1086)
==25136==    by 0x4E4B348: __pmFindOrOpenArchive (context.c:555)
==25136==    by 0x4E4CAF1: initarchive (context.c:759)
==25136==    by 0x4E4CAF1: pmNewContext (context.c:1126)
==25136==    by 0x4A603B: QmcSource::retryConnect(int, QString&amp;) (in /usr/bin/pmchart)
==25136==    by 0x4A6F16: QmcSource::QmcSource(int, QString&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x4A71C6: QmcSource::getSource(int, QString&amp;, int, bool) (in /usr/bin/pmchart)
==25136==    by 0x49C1B1: QmcGroup::use(int, QString const&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x424DB0: main (in /usr/bin/pmchart)
==25136==  Address 0x1f35e370 is 0 bytes inside an unallocated block of size 112 in arena &quot;client&quot;
==25136== 
==25136== Invalid free() / delete / delete[] / realloc()
==25136==    at 0x4C2CD5A: free (vg_replace_malloc.c:530)
==25136==    by 0x4E68260: logFreePMNS (logutil.c:707)
==25136==    by 0x4E6A393: __pmLogOpen (logutil.c:1107)
==25136==    by 0x4E4B348: __pmFindOrOpenArchive (context.c:555)
==25136==    by 0x4E4CAF1: initarchive (context.c:759)
==25136==    by 0x4E4CAF1: pmNewContext (context.c:1126)
==25136==    by 0x4A603B: QmcSource::retryConnect(int, QString&amp;) (in /usr/bin/pmchart)
==25136==    by 0x4A6F16: QmcSource::QmcSource(int, QString&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x4A71C6: QmcSource::getSource(int, QString&amp;, int, bool) (in /usr/bin/pmchart)
==25136==    by 0x49C1B1: QmcGroup::use(int, QString const&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x424DB0: main (in /usr/bin/pmchart)
==25136==  Address 0x1f361640 is 0 bytes inside a block of size 20 free&apos;d
==25136==    at 0x4C2CD5A: free (vg_replace_malloc.c:530)
==25136==    by 0x4E65C59: __pmLogLoadMeta (logmeta.c:273)
==25136==    by 0x4E6A43F: __pmLogOpen (logutil.c:1086)
==25136==    by 0x4E4B348: __pmFindOrOpenArchive (context.c:555)
==25136==    by 0x4E4CAF1: initarchive (context.c:759)
==25136==    by 0x4E4CAF1: pmNewContext (context.c:1126)
==25136==    by 0x4A603B: QmcSource::retryConnect(int, QString&amp;) (in /usr/bin/pmchart)
==25136==    by 0x4A6F16: QmcSource::QmcSource(int, QString&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x4A71C6: QmcSource::getSource(int, QString&amp;, int, bool) (in /usr/bin/pmchart)
==25136==    by 0x49C1B1: QmcGroup::use(int, QString const&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x424DB0: main (in /usr/bin/pmchart)
==25136==  Block was alloc&apos;d at
==25136==    at 0x4C2BBAD: malloc (vg_replace_malloc.c:299)
==25136==    by 0x4E65B92: __pmLogLoadMeta (logmeta.c:208)
==25136==    by 0x4E6A43F: __pmLogOpen (logutil.c:1086)
==25136==    by 0x4E4B348: __pmFindOrOpenArchive (context.c:555)
==25136==    by 0x4E4CAF1: initarchive (context.c:759)
==25136==    by 0x4E4CAF1: pmNewContext (context.c:1126)
==25136==    by 0x4A603B: QmcSource::retryConnect(int, QString&amp;) (in /usr/bin/pmchart)
==25136==    by 0x4A6F16: QmcSource::QmcSource(int, QString&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x4A71C6: QmcSource::getSource(int, QString&amp;, int, bool) (in /usr/bin/pmchart)
==25136==    by 0x49C1B1: QmcGroup::use(int, QString const&amp;, int) (in /usr/bin/pmchart)
==25136==    by 0x424DB0: main (in /usr/bin/pmchart)
==25136==</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9958785</commentid>
    <comment_count>14</comment_count>
    <who name="Mark Goodwin">mgoodwin</who>
    <bug_when>2016-12-06 01:41:32 -0500</bug_when>
    <thetext>I backed out 6ed4b2452ff24356f90d37ac658cdbae4053e8c0 from my tree and rebuilt just with Dave&apos;s patch - it&apos;s much better behaved, even works actually, doesn&apos;t spin nor get killed like it used to using the repro from Comment #0

I think this patch is a tremendous improvement. 6ed4b2452ff24 can be reverted since on inspection it&apos;s included in Dave&apos;s patch (hence the double free reported in previous comment)

-- Mark</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9960883</commentid>
    <comment_count>15</comment_count>
    <who name="Dave Brolley">brolley</who>
    <bug_when>2016-12-06 10:28:34 -0500</bug_when>
    <thetext>Both leaks must be fixed in the final version. I&apos;ll review and make sure that&apos;s the case.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9966191</commentid>
    <comment_count>16</comment_count>
    <who name="Mark Goodwin">mgoodwin</who>
    <bug_when>2016-12-07 18:11:16 -0500</bug_when>
    <thetext>Dave&apos;s current patch - a quick test shows it seems to work

In the tree/branch:  git://git.pcp.io/brolley/pcp rhbz1359975

commit 0015487d1fd1c70f94bb75b08109a13de81dc878
Author: Dave Brolley &lt;brolley@redhat.com&gt;
Date:   Wed Dec 7 11:50:40 2016 -0500

    RHBZ 1359975: pmchart run-away mem leak replaying multi-archive when rewinding
    
    Two leaks were fixed. Both occurred during the transition between archives
    is a multi-archive context.
    
    1) a leak of pmDesc * used to check for consistency of the PMNS
       between archives.
    
    2) a build up of duplicate instance domains.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9978108</commentid>
    <comment_count>17</comment_count>
    <who name="Dave Brolley">brolley</who>
    <bug_when>2016-12-12 14:50:08 -0500</bug_when>
    <thetext>Additional proposed commit (on top of the previous one):

In the tree/branch:  git://git.pcp.io/brolley/pcp rhbz1359975

commit 82e02ffea204d26e7617a4bcf4be0b9326c64457
Author: Dave Brolley &lt;brolley@redhat.com&gt;
Date:   Mon Dec 12 14:39:09 2016 -0500

    RHBZ 1359975: pmchart run-away mem leak replaying multi-archive when rewinding
    
    Refinement. Exposed by qa regressions. Ensure that instance domains are in
    the correct order in the hash chain:
    
    - Primary sort by timestamp (descending)
    - Secondary sort: latest added at the head of each time slot. This
      includes moving duplicates to the head of their time slot when detected.
    
    Before multi-archive contexts and duplicate-filtering, this happened
    automatically.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9986533</commentid>
    <comment_count>18</comment_count>
    <who name="Dave Brolley">brolley</who>
    <bug_when>2016-12-14 11:19:19 -0500</bug_when>
    <thetext>More commits for this bug:

In the tree/branch:  git://git.pcp.io/brolley/pcp rhbz1359975

commit e574b71de8420c719ae2924a64ca9a5a4411bd52
Author: Dave Brolley &lt;brolley@redhat.com&gt;
Date:   Wed Dec 14 11:10:45 2016 -0500

    RHBZ 1359975: pmchart run-away mem leak replaying multi-archive when rewinding.
    
    Don&apos;t assume that instances within the same instance domain are
    always sorted in the same order.

commit f32bf1e7a669a2bee719e9d3f6c3828acfb8c518
Author: Dave Brolley &lt;brolley@redhat.com&gt;
Date:   Tue Dec 13 12:35:30 2016 -0500

    Create and use __pmTimevalCmp for comparing __pmTimeval.
    
    Similar to __pmtimevalcmp which compares struct timeval.

commit af8cac3b71b3104157a268c6ebb6c652c87d01ca
Author: Nathan Scott &lt;nathans@redhat.com&gt;
Date:   Tue Dec 13 12:06:52 2016 -0500

    Typos and style corrections for recent duplicate __pmLogInDom changes.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>9987254</commentid>
    <comment_count>19</comment_count>
    <who name="Dave Brolley">brolley</who>
    <bug_when>2016-12-14 14:00:43 -0500</bug_when>
    <thetext>All commits have been pushed upstream to git://git.pcp.io/pcp master</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10012705</commentid>
    <comment_count>20</comment_count>
    <who name="Fedora Update System">updates</who>
    <bug_when>2016-12-21 19:41:41 -0500</bug_when>
    <thetext>pcp-3.11.7-1.el5 has been submitted as an update to Fedora EPEL 5. https://bodhi.fedoraproject.org/updates/FEDORA-EPEL-2016-10df136b72</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10012708</commentid>
    <comment_count>21</comment_count>
    <who name="Fedora Update System">updates</who>
    <bug_when>2016-12-21 19:43:44 -0500</bug_when>
    <thetext>pcp-3.11.7-1.fc24 has been submitted as an update to Fedora 24. https://bodhi.fedoraproject.org/updates/FEDORA-2016-98254aaaa3</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10012711</commentid>
    <comment_count>22</comment_count>
    <who name="Fedora Update System">updates</who>
    <bug_when>2016-12-21 19:45:07 -0500</bug_when>
    <thetext>pcp-3.11.7-1.fc25 has been submitted as an update to Fedora 25. https://bodhi.fedoraproject.org/updates/FEDORA-2016-9cbe6566bd</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10016935</commentid>
    <comment_count>23</comment_count>
    <who name="Fedora Update System">updates</who>
    <bug_when>2016-12-22 14:19:19 -0500</bug_when>
    <thetext>pcp-3.11.7-1.fc24 has been pushed to the Fedora 24 testing repository. If problems still persist, please make note of it in this bug report.
See https://fedoraproject.org/wiki/QA:Updates_Testing for
instructions on how to install test updates.
You can provide feedback for this update here: https://bodhi.fedoraproject.org/updates/FEDORA-2016-98254aaaa3</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10016951</commentid>
    <comment_count>24</comment_count>
    <who name="Fedora Update System">updates</who>
    <bug_when>2016-12-22 14:20:35 -0500</bug_when>
    <thetext>pcp-3.11.7-1.fc25 has been pushed to the Fedora 25 testing repository. If problems still persist, please make note of it in this bug report.
See https://fedoraproject.org/wiki/QA:Updates_Testing for
instructions on how to install test updates.
You can provide feedback for this update here: https://bodhi.fedoraproject.org/updates/FEDORA-2016-9cbe6566bd</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10017090</commentid>
    <comment_count>25</comment_count>
    <who name="Fedora Update System">updates</who>
    <bug_when>2016-12-22 15:47:04 -0500</bug_when>
    <thetext>pcp-3.11.7-1.el5 has been pushed to the Fedora EPEL 5 testing repository. If problems still persist, please make note of it in this bug report.
See https://fedoraproject.org/wiki/QA:Updates_Testing for
instructions on how to install test updates.
You can provide feedback for this update here: https://bodhi.fedoraproject.org/updates/FEDORA-EPEL-2016-10df136b72</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10028963</commentid>
    <comment_count>26</comment_count>
    <who name="Fedora Update System">updates</who>
    <bug_when>2017-01-02 14:50:59 -0500</bug_when>
    <thetext>pcp-3.11.7-1.fc25 has been pushed to the Fedora 25 stable repository. If problems still persist, please make note of it in this bug report.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10029269</commentid>
    <comment_count>27</comment_count>
    <who name="Fedora Update System">updates</who>
    <bug_when>2017-01-02 20:21:30 -0500</bug_when>
    <thetext>pcp-3.11.7-1.fc24 has been pushed to the Fedora 24 stable repository. If problems still persist, please make note of it in this bug report.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10042035</commentid>
    <comment_count>28</comment_count>
    <who name="Fedora Update System">updates</who>
    <bug_when>2017-01-06 18:48:39 -0500</bug_when>
    <thetext>pcp-3.11.7-1.el5 has been pushed to the Fedora EPEL 5 stable repository. If problems still persist, please make note of it in this bug report.</thetext>
  </long_desc>
      
          <attachment
              isobsolete="0"
              ispatch="0"
              isprivate="0"
          >
            <attachid>1225074</attachid>
            <date>2016-11-27 20:47:00 -0500</date>
            <delta_ts>2016-11-27 20:47:55 -0500</delta_ts>
            <desc>possible reproducer</desc>
            <filename>pmbackval.py</filename>
            <type>text/plain</type>
            <size>1566</size>
            <attacher name="Frank Ch. Eigler">fche</attacher>
            
              <data encoding="base64">IyEvdXNyL2Jpbi9lbnYgcG1weXRob24KIwojIGdldCBweXRob24zLWNvbXBhdGlibGUgaW5jcmVt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</data>

          </attachment>
          <attachment
              isobsolete="0"
              ispatch="1"
              isprivate="0"
          >
            <attachid>1225100</attachid>
            <date>2016-11-27 22:57:00 -0500</date>
            <delta_ts>2016-11-27 22:57:59 -0500</delta_ts>
            <desc>fix for one small leak</desc>
            <filename>k</filename>
            <type>text/plain</type>
            <size>641</size>
            <attacher name="Frank Ch. Eigler">fche</attacher>
            
              <data encoding="base64">ZGlmZiAtLWdpdCBhL3NyYy9saWJwY3Avc3JjL2xvZ21ldGEuYyBiL3NyYy9saWJwY3Avc3JjL2xv
Z21ldGEuYwppbmRleCAwMzEwYWVhOGUyOTMuLjkwNGM3YjIzNmNhNiAxMDA2NDQKLS0tIGEvc3Jj
L2xpYnBjcC9zcmMvbG9nbWV0YS5jCisrKyBiL3NyYy9saWJwY3Avc3JjL2xvZ21ldGEuYwpAQCAt
MjAwLDYgKzIwMCwxMyBAQCBQTV9GQVVMVF9QT0lOVCgibGlicGNwLyIgX19GSUxFX18gIjoyIiwg
UE1fRkFVTFRfQUxMT0MpOwogCQkgICAgZnJlZShkcCk7CiAJCSAgICBnb3RvIGVuZDsKIAkJfQor
CisgICAgICAgICAgICAgICAgLyoKKyAgICAgICAgICAgICAgICAgKiBUaGlzIHBtaWQgaXMgYWxy
ZWFkeSBrbm93biwgYW5kIG1hdGNoZXMuICBXZSBjYW4gZnJlZSB0aGUgbmV3bHkKKyAgICAgICAg
ICAgICAgICAgKiByZWFkIGNvcHkgYW5kIHVzZSB0aGUgb25lIGluIHRoZSBoYXNoIHRhYmxlLiAK
KyAgICAgICAgICAgICAgICAgKi8KKyAgICAgICAgICAgICAgICBmcmVlKGRwKTsKKyAgICAgICAg
ICAgICAgICBkcCA9IG9sZGRwOwogCSAgICB9CiAJICAgIGVsc2UgaWYgKChzdHMgPSBfX3BtSGFz
aEFkZCgoaW50KWRwLT5wbWlkLCAodm9pZCAqKWRwLCAmbGNwLT5sX2hhc2hwbWlkKSkgPCAwKSB7
CiAJCWZyZWUoZHApOwo=
</data>

          </attachment>
          <attachment
              isobsolete="0"
              ispatch="1"
              isprivate="0"
          >
            <attachid>1227374</attachid>
            <date>2016-12-02 10:46:00 -0500</date>
            <delta_ts>2016-12-02 10:46:21 -0500</delta_ts>
            <desc>Proposed patch for indom duplication</desc>
            <filename>1359975-2.patch</filename>
            <type>text/plain</type>
            <size>3532</size>
            <attacher name="Dave Brolley">brolley</attacher>
            
              <data encoding="base64">ZGlmZiAtLWdpdCBhL3NyYy9saWJwY3Avc3JjL2xvZ21ldGEuYyBiL3NyYy9saWJwY3Avc3JjL2xv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==
</data>

          </attachment>
      

    </bug>

</bugzilla>