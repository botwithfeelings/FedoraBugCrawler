<?xml version="1.0" encoding="UTF-8" standalone="yes" ?>
<!DOCTYPE bugzilla SYSTEM "https://bugzilla.redhat.com/page.cgi?id=bugzilla.dtd">

<bugzilla version="4.4.12068.1"
          urlbase="https://bugzilla.redhat.com/"
          
          maintainer="bugzilla-error-list@redhat.com"
>

    <bug>
          <bug_id>1426477</bug_id>
          
          <creation_ts>2017-02-23 21:14:00 -0500</creation_ts>
          <short_desc>libvirtd crashes when calling virConnectGetAllDomainStats on a VM which has empty cdrom drive</short_desc>
          <delta_ts>2017-05-04 13:49:38 -0400</delta_ts>
          <reporter_accessible>1</reporter_accessible>
          <cclist_accessible>1</cclist_accessible>
          <classification_id>2</classification_id>
          <classification>Fedora</classification>
          <product>Fedora</product>
          <component>libvirt</component>
          <version>26</version>
          <rep_platform>x86_64</rep_platform>
          <op_sys>Linux</op_sys>
          <bug_status>CLOSED</bug_status>
          <resolution>CURRENTRELEASE</resolution>
          
          
          <bug_file_loc></bug_file_loc>
          <status_whiteboard></status_whiteboard>
          <keywords>Regression, Reopened, TestBlocker</keywords>
          <priority>unspecified</priority>
          <bug_severity>high</bug_severity>
          <target_milestone>---</target_milestone>
          <dependson>1420718</dependson>
          
          <everconfirmed>1</everconfirmed>
          <reporter name="Han Han">hhan</reporter>
          <assigned_to name="Libvirt Maintainers">libvirt-maint</assigned_to>
          <cc>agedosier</cc>
    
    
    <cc>ahadas</cc>
    
    
    <cc>berrange</cc>
    
    
    <cc>bugs</cc>
    
    
    <cc>chhu</cc>
    
    
    <cc>clalancette</cc>
    
    
    <cc>crobinso</cc>
    
    
    <cc>dyuan</cc>
    
    
    <cc>hhan</cc>
    
    
    <cc>itamar</cc>
    
    
    <cc>laine</cc>
    
    
    <cc>libvirt-maint</cc>
    
    
    <cc>michal.skrivanek</cc>
    
    
    <cc>mzhan</cc>
    
    
    <cc>pkrempa</cc>
    
    
    <cc>pzhang</cc>
    
    
    <cc>rbalakri</cc>
    
    
    <cc>veillard</cc>
    
    
    <cc>virt-maint</cc>
    
    
    <cc>xuzhang</cc>
    
    
    <cc>yanqzhan</cc>
          <qa_contact name="Fedora Extras Quality Assurance">extras-qa</qa_contact>
          
          <cf_fixed_in></cf_fixed_in>
          <cf_doc_type>If docs needed, set a value</cf_doc_type>
          <cf_release_notes></cf_release_notes>
          <cf_story_points>---</cf_story_points>
          <cf_clone_of>1420718</cf_clone_of>
          <cf_environment></cf_environment>
          <cf_last_closed>2017-05-04 13:49:38</cf_last_closed>
          <cf_type>Bug</cf_type>
          <cf_regression_status>---</cf_regression_status>
          <cf_mount_type>---</cf_mount_type>
          <cf_documentation_action>---</cf_documentation_action>
          <cf_crm></cf_crm>
          <cf_verified_branch></cf_verified_branch>
          <cf_category>---</cf_category>
          <cf_ovirt_team>Virt</cf_ovirt_team>
          
          <cf_cloudforms_team>---</cf_cloudforms_team>
          
          
          
          
          <target_release>---</target_release>
          
          <votes>0</votes>

      

      

      

          <comment_sort_order>oldest_to_newest</comment_sort_order>  
          <long_desc isprivate="0" >
    <commentid>10180550</commentid>
    <comment_count>0</comment_count>
      <attachid>1257114</attachid>
    <who name="Han Han">hhan</who>
    <bug_when>2017-02-23 21:14:00 -0500</bug_when>
    <thetext>Created attachment 1257114
Use virsh to reproduce

This bug could be reproduced on libvirt-3.1.0-1.fc26.x86_64

+++ This bug was initially created as a clone of Bug #1420718 +++

Description of problem:
Fail to initiate a console session for VM when RunOnce by network(pxe) 

Version-Release number of selected component (if applicable):
Rhevm-4.1.0.4-0.1.el7.noarch
Hosts:
libvirt-daemon-3.0.0-1.el7.x86_64
vdsm-4.19.5-1.el7ev.x86_64

How reproducible:
100%

Steps to Reproduce:
1.Create a data center: edit name, set others as default.

2.Create a cluster: edit name, CPU Arc:x86_64, CPU Type: AMD, set others as default.

3.Add 2 hosts with AMD cpu

4.Create a nfs storage

5.New a vm: edit name, nic1:ovritmgmt/ovritmgmt, 
            create image: 5G, Interface:VirtIO, storage:nfs
set others as default.

6.RunOnce vm: boot options: select “network(pxe)” up to top, click ‘ok’ .

7.When vm is PoweringUp, click ‘console’ icon to open console for vm.


Actual results:
The console for vm is opened and begin booting from pxe for a few seconds, but will be closed soon.
Click the ‘console’ icon again to reopen console, console cannot be opened with error: “Setting VM ticket failed”.
After a few minutes, vm is down.


Expected results:
The console should keep open for booting, should not get error “Setting VM ticket failed”, and the vm should not be down.


Additional info:
1.Pls refer to attachments for logs.
2.Not occurred on Rhevm-4.1.0-0.3.beta2.el7.noarch, vdsm-4.19.1-1.el7ev.x86_64.
3.“Setting VM ticket failed” also occurred when: after &apos;Runonce&apos; installation with boot.iso then &apos;Run&apos; VM and try to open console. VM will also down.

--- Additional comment from yanqzhan@redhat.com on 2017-02-09 06:36 EST ---



--- Additional comment from Arik on 2017-02-09 07:01:03 EST ---

I don&apos;t see anything useful in the attached engine.log - are you sure it covers the right time?
Can you provide more information - what was the name of the VM, when did you notice that it went Down or failed to initiate a console?

--- Additional comment from yanqzhan@redhat.com on 2017-02-09 22:29 EST ---

The VM name is &apos;AA&apos;. On rhevm webpage, the events are as follows:
  VM AA was started by admin@internal-authz (Host: A).
  User admin@internal-authz initiated console session for VM AA
  User admin@internal-authz is connected to VM AA.
  VDSM A command GetStatsVDS failed: Heartbeat exceeded
  VDSM A command SpmStatusVDS failed: Heartbeat exceeded
  User admin@internal-authz failed to initiate a console session for VM AA
  Invalid status on Data Center Default. Setting Data Center status to Non Responsive (On host A, Error: Network error during communication with the Host.).
  User admin@internal-authz failed to initiate a console session for VM AA
  User admin@internal-authz failed to initiate a console session for VM AA
  VDSM command GetStoragePoolInfoVDS failed: Heartbeat exceeded
  Status of host A was set to Up.
  VDSM A command HSMGetAllTasksStatusesVDS failed: Not SPM
  VM AA is down with error. Exit message: Failed to find the libvirt domain.
  Failed to run VM AA on Host A.
  Failed to run VM AA (User: admin@internal-authz).
  Storage Pool Manager runs on Host A (Address: amd-8750-4-2.englab.nay.redhat.com).

Pls refer to the new attachment: engineLog_errorPicture for more details.

--- Additional comment from Michal Skrivanek on 2017-02-10 02:33:35 EST ---

there are libvirt connectivity issues too - please get libvirt, qemu, and system logs and specify what are the exact pkgs versions

--- Additional comment from yanqzhan@redhat.com on 2017-02-10 03:36 EST ---

Pls refer to new attachment:&quot;libvirt_qemu_messages&quot;,
 including /var/log/libvirt/libvirtd.log,
           /var/log/libvirt/qemu/AA.log,
           /var/log/messages

pkgs version: libvirt-daemon-3.0.0-1.el7.x86_64
             qemu-kvm-rhev-2.8.0-3.el7.x86_64
             kernel-3.10.0-557.el7.x86_64

--- Additional comment from Michal Skrivanek on 2017-02-10 04:03:09 EST ---

note the qemu log in the attachment is not a qemu log but libvirt log. Please update

In general there seems to be network connectivity issues

--- Additional comment from yanqzhan@redhat.com on 2017-02-14 07:30 EST ---

Hi, 

    The AA.log is indeed qemu log under /var/log/libvirt/qemu/.
    But for your request, I reproduced with another env, the new attachment &quot;yan-V3.log&quot; is generated by same steps and with same pkgs version. 
    Hope this file be helpful for you, since the old test env is cleaned for new testing now.

    There&apos;re &quot;shutting down, reason=crashed&quot; info in log, maybe it seems not a network issue.

--- Additional comment from yanqzhan@redhat.com on 2017-02-22 06:17 EST ---

It&apos;s a libvirt issue, reproduce on both libvirt-3.0.0-2.el7.x86_64 and libvirt-3.0.0-1.el7.x86_64, not reproduced on libvirt-2.5.0-1.el7.x86_64.

Pls refer to attachment: gdb.txt for libvirtd

--- Additional comment from Red Hat Bugzilla Rules Engine on 2017-02-22 06:38:51 EST ---

Since this bug report was entered in Red Hat Bugzilla, the release flag has been set to ? to ensure that it is properly evaluated for this release.

--- Additional comment from Red Hat Bugzilla Rules Engine on 2017-02-22 06:38:51 EST ---

This bug report has Keywords: Regression or TestBlocker.

Since no regressions or test blockers are allowed between releases, it is also being [proposed|marked] as a blocker for this release.

Please resolve ASAP.

--- Additional comment from yanqzhan@redhat.com on 2017-02-23 01:06:15 EST ---

Hi, can reproduce on following env:
rhevm-4.0.7-0.1.el7ev.noarch
vdsm-4.18.23-1.el7ev.x86_64
Libvirt-3.0.0-2.el7.x86_64

Steps:
1.gdb attaches libvirtd process
#gdb -p `pidof libvirtd` 
(gdb)c
Continuing

2.Try to start the vm by RunOnce with network(pxe) on rhevm webpage

3.A coredump occurred:
(gdb)
Continuing
Detaching after fork from child process 12902.
...
Detaching after fork from child process 13012.
Program received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7ff34ab84700 (LWP 12530)]
__strrchr_sse42 () at ../sysdeps/x86_64/multiarch/strrchr.S:138
138        pcmpistri    $0x4a, (%r8), %xmm1


So guess that it&apos;s libvirtd bug. For more details pls refer to the attachment:&quot;gdb.txt for libvirtd&quot;



backtrace of libvirtd:
(gdb) bt
#0  __strrchr_sse42 () at ../sysdeps/x86_64/multiarch/strrchr.S:138
#1  0x00007ff35b4f1ec3 in virFileIsSharedFSType (path=path@entry=0x0,
    fstypes=fstypes@entry=63) at util/virfile.c:3363
#2  0x00007ff35b4f283a in virFileIsSharedFS (path=path@entry=0x0)
    at util/virfile.c:3569
#3  0x00007ff32fe08864 in qemuOpenFileAs (bypassSecurityDriver=0x0,
    needUnlink=0x0, oflags=0, path=0x0, dynamicOwnership=false,
    fallback_gid=107, fallback_uid=107) at qemu/qemu_driver.c:2927
#4  qemuOpenFile (driver=driver@entry=0x7ff3241167d0,
    vm=vm@entry=0x7ff320018b10, path=0x0, oflags=oflags@entry=0,
    needUnlink=needUnlink@entry=0x0,
    bypassSecurityDriver=bypassSecurityDriver@entry=0x0)
    at qemu/qemu_driver.c:2908
#5  0x00007ff32fe26c73 in qemuDomainStorageOpenStat (
    driver=driver@entry=0x7ff3241167d0, vm=vm@entry=0x7ff320018b10,
    src=src@entry=0x7ff320017140, ret_fd=ret_fd@entry=0x7ff34ab836ac,
    ret_sb=ret_sb@entry=0x7ff34ab836b0, cfg=0x7ff32410bca0, cfg=0x7ff32410bca0)
    at qemu/qemu_driver.c:11602
#6  0x00007ff32fe26ff0 in qemuDomainStorageUpdatePhysical (
    driver=driver@entry=0x7ff3241167d0, cfg=cfg@entry=0x7ff32410bca0,
    vm=vm@entry=0x7ff320018b10, src=src@entry=0x7ff320017140)
    at qemu/qemu_driver.c:11655
#7  0x00007ff32fe27a5d in qemuDomainGetStatsOneBlock (
---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---
    driver=driver@entry=0x7ff3241167d0, cfg=cfg@entry=0x7ff32410bca0,
    dom=dom@entry=0x7ff320018b10, record=record@entry=0x7ff318007de0,
    maxparams=maxparams@entry=0x7ff34ab839e4, src=src@entry=0x7ff320017140,
    block_idx=block_idx@entry=0, backing_idx=backing_idx@entry=0,
    stats=0x7ff318008060, disk=0x7ff320016f90, disk=0x7ff320016f90)
    at qemu/qemu_driver.c:19541
#8  0x00007ff32fe27ecc in qemuDomainGetStatsBlock (driver=0x7ff3241167d0,
    dom=0x7ff320018b10, record=0x7ff318007de0, maxparams=0x7ff34ab839e4,
    privflags=&lt;optimized out&gt;) at qemu/qemu_driver.c:19600
#9  0x00007ff32fe0cb21 in qemuDomainGetStats (flags=1,
    record=&lt;synthetic pointer&gt;, stats=127, dom=0x7ff320018b10,
    conn=0x7ff3241efcb0) at qemu/qemu_driver.c:19762
#10 qemuConnectGetAllDomainStats (conn=0x7ff3241efcb0, doms=&lt;optimized out&gt;,
    ndoms=&lt;optimized out&gt;, stats=127, retStats=0x7ff34ab83b10,
    flags=&lt;optimized out&gt;) at qemu/qemu_driver.c:19852
#11 0x00007ff35b5f5dc6 in virConnectGetAllDomainStats (conn=0x7ff3241efcb0,
    stats=0, retStats=retStats@entry=0x7ff34ab83b10, flags=0)
    at libvirt-domain.c:11311
#12 0x00007ff35c252d30 in remoteDispatchConnectGetAllDomainStats (
    server=0x7ff35e131020, msg=0x7ff35e164560, ret=0x7ff318007d80,
    args=0x7ff318006150, rerr=0x7ff34ab83c50, client=0x7ff35e1611e0)
    at remote.c:6543
#13 remoteDispatchConnectGetAllDomainStatsHelper (server=0x7ff35e131020,
---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---
    client=0x7ff35e1611e0, msg=0x7ff35e164560, rerr=0x7ff34ab83c50,
    args=0x7ff318006150, ret=0x7ff318007d80) at remote_dispatch.h:615
#14 0x00007ff35b656072 in virNetServerProgramDispatchCall (msg=0x7ff35e164560,
    client=0x7ff35e1611e0, server=0x7ff35e131020, prog=0x7ff35e1510f0)
    at rpc/virnetserverprogram.c:437
#15 virNetServerProgramDispatch (prog=0x7ff35e1510f0,
    server=server@entry=0x7ff35e131020, client=0x7ff35e1611e0,
    msg=0x7ff35e164560) at rpc/virnetserverprogram.c:307
#16 0x00007ff35c285ccd in virNetServerProcessMsg (msg=&lt;optimized out&gt;,
    prog=&lt;optimized out&gt;, client=&lt;optimized out&gt;, srv=0x7ff35e131020)
    at rpc/virnetserver.c:148
#17 virNetServerHandleJob (jobOpaque=&lt;optimized out&gt;, opaque=0x7ff35e131020)
    at rpc/virnetserver.c:169
#18 0x00007ff35b53d401 in virThreadPoolWorker (
    opaque=opaque@entry=0x7ff35e125bb0) at util/virthreadpool.c:167
#19 0x00007ff35b53c788 in virThreadHelper (data=&lt;optimized out&gt;)
    at util/virthread.c:206
#20 0x00007ff3588ffdc5 in start_thread (arg=0x7ff34ab84700)
    at pthread_create.c:308
#21 0x00007ff3586277ad in clone ()
    at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113

--- Additional comment from Peter Krempa on 2017-02-23 04:23:49 EST ---

Way simpler libvirt-only reproducer is to create a VM with an empty cdrom drive and call &quot;virsh domstats VM&quot; while it&apos;s running.

--- Additional comment from Han Han on 2017-02-23 21:07 EST ---

Reproded via virsh:
➜  ~ virsh attach-disk V  /tmp/boot.iso sda --config --type cdrom
Disk attached successfully

➜  ~ virsh change-media V sda --eject --config
Successfully ejected media.
➜  ~ virsh start V
Domain V started

➜  ~ virsh domstats V
error: Disconnected from qemu:///system due to I/O error
error: End of file while reading data: Input/output error</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10180557</commentid>
    <comment_count>1</comment_count>
    <who name="Han Han">hhan</who>
    <bug_when>2017-02-23 21:16:57 -0500</bug_when>
    <thetext>Version:
libvirt-3.1.0-1.fc26.x86_64

Reproduced via virsh:
➜  ~ virsh attach-disk V  /tmp/boot.iso sda --config --type cdrom
Disk attached successfully

➜  ~ virsh change-media V sda --eject --config
Successfully ejected media.
➜  ~ virsh start V
Domain V started

➜  ~ virsh domstats V
error: Disconnected from qemu:///system due to I/O error
error: End of file while reading data: Input/output error</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10181082</commentid>
    <comment_count>2</comment_count>
    <who name="Peter Krempa">pkrempa</who>
    <bug_when>2017-02-24 03:48:08 -0500</bug_when>
    <thetext>Fedora bugs with all private comments are useless. Please file one with proper data and trim the private information.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10181462</commentid>
    <comment_count>3</comment_count>
    <who name="Peter Krempa">pkrempa</who>
    <bug_when>2017-02-24 07:27:33 -0500</bug_when>
    <thetext>Upstream fixed this as:

commit c3de387380f6057ee0e46cd9f2f0a092e8070875
Author: Peter Krempa &lt;pkrempa@redhat.com&gt;
Date:   Thu Feb 23 10:07:30 2017 +0100

    qemu: Don&apos;t update physical storage size of empty drives
    
    Previously the code called virStorageSourceUpdateBlockPhysicalSize which
    did not do anything on empty drives since it worked only on block
    devices. After the refactor in c5f6151390 it&apos;s called for all devices
    and thus attempts to deref the NULL path of empty drives.
    
    Add a check that skips the update of the physical size if the storage
    source is empty.
    
    Resolves: https://bugzilla.redhat.com/show_bug.cgi?id=1420718</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10193422</commentid>
    <comment_count>5</comment_count>
    <who name="Fedora End Of Life">jkurik</who>
    <bug_when>2017-02-28 07:25:50 -0500</bug_when>
    <thetext>This bug appears to have been reported against &apos;rawhide&apos; during the Fedora 26 development cycle.
Changing version to &apos;26&apos;.</thetext>
  </long_desc><long_desc isprivate="0" >
    <commentid>10387007</commentid>
    <comment_count>6</comment_count>
    <who name="Cole Robinson">crobinso</who>
    <bug_when>2017-05-04 13:49:38 -0400</bug_when>
    <thetext>This has been in f26+ since libvirt 3.1.0</thetext>
  </long_desc>
      
      

    </bug>

</bugzilla>